{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import os \n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import  utils\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import p3_model2 as model \n",
    "\n",
    "#import p3_hw as hw\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "from itertools import cycle\n",
    "import torch.optim as optim\n",
    "import p3_test2 as test\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_domain='real' # change here \n",
    "#domain_name=['sketch','quickdraw','infograph','real']\n",
    "domain_name=['infograph']\n",
    "#domain_name.remove(target_domain)\n",
    "\n",
    "class_name=[file  for file in os.listdir(domain_name[0]) if file[-3:] !='csv' ]\n",
    "\n",
    "csv_name_train={name: pd.read_csv(cwd+'/'+name+'/'+name+ '_train.csv',index_col=0) for name in domain_name}\n",
    "csv_name_val={target_domain: pd.read_csv(cwd+'/'+target_domain+'/'+target_domain+ '_train.csv',index_col=0)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cvs=pd.concat(csv_name_train[key] for key in domain_name)\n",
    "test_csv=csv_name_val[target_domain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_transform=transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "class finalset(Dataset):\n",
    "    def __init__(self, train_cvs,train, transform=None):\n",
    "        \"\"\" Intialize the MNIST dataset \"\"\"\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "        self.csv=train_cvs\n",
    "        self.filenames = list(train_cvs.index)\n",
    "        self.train=train\n",
    "        self.transform = transform\n",
    "        self.len = len(self.filenames)                      \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        \"\"\" Get a sample from the dataset \"\"\"\n",
    "        \n",
    "        image_fn=self.filenames[index]\n",
    "        image = Image.open(image_fn)\n",
    "    \n",
    "        if image.mode != 'RGB':\n",
    "            image = np.expand_dims(image, axis=2)\n",
    "            image=np.concatenate((image,image,image),axis=2)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.train is True:\n",
    "            label=self.csv.loc[image_fn,'label']\n",
    "            return image,label,image_fn\n",
    "        if self.train is False:\n",
    "            return image, -1, image_fn\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len  \n",
    "\n",
    "train_data  =finalset(train_cvs,train=True,transform=color_transform)\n",
    "trainloader = DataLoader(train_data, batch_size=64,shuffle=True) \n",
    "\n",
    "\n",
    "test_data  =finalset(test_csv,train=True,transform=color_transform)\n",
    "testloader = DataLoader(test_data, batch_size=64,shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_size=2048\n",
    "cnn_feature_extractor=torchvision.models.resnet50(pretrained=True) #resnet50 fc is for 1000 calsses\n",
    "modules = list(cnn_feature_extractor.children())[:-1] # delete the last fc layer.\n",
    "cnn_feature_extractor = nn.Sequential(*modules).to(device)\n",
    "\n",
    "# set requires_grad to false\n",
    "for param in cnn_feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "#encoder=model.SVHN_Extractor().cuda()\n",
    "encoder=cnn_feature_extractor.cuda()\n",
    "classifier=model.SVHN_Class_classifier().cuda()\n",
    "discriminator=model.SVHN_Domain_classifier().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(encoder,classifier,discriminator,save_name):\n",
    "    print('Save models  ...')\n",
    "\n",
    "    save_folder = 'models'\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    state_enc={\"state_dict\": encoder.state_dict()}\n",
    "    torch.save(state_enc, str(save_folder) + '/' + str(save_name) + '_enc.pth')\n",
    "    \n",
    "    state_cls={\"state_dict\": classifier.state_dict()}\n",
    "    torch.save(state_cls, str(save_folder) + '/' + str(save_name)  +'_cls.pth')\n",
    "    print(' p3 Model '+ str(save_name)+' is saved !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANN training.......\n",
      "Epoch : 0\n",
      "1.6967058181762695\n",
      "3.5542778968811035\n",
      "5.331728935241699\n",
      "6.896262884140015\n"
     ]
    }
   ],
   "source": [
    "\n",
    "srt_time = time.time()\n",
    "epochs=40\n",
    "max_target_acc=0\n",
    "for epoch in range(epochs):\n",
    "    print(\"DANN training.......\")\n",
    "    print('Epoch : {}'.format(epoch))\n",
    "\n",
    "    encoder = encoder.train()\n",
    "    classifier = classifier.train()\n",
    "    discriminator = discriminator.train()\n",
    "\n",
    "    classifier_criterion = nn.NLLLoss().cuda()#nn.CrossEntropyLoss().cuda()\n",
    "    discriminator_criterion = nn.NLLLoss().cuda()#nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    start_steps = epoch * len(trainloader)\n",
    "    total_steps = epochs * len(testloader)\n",
    "    lens_use=max(len(trainloader),len(testloader))\n",
    "    #print(time.time()-srt_time)\n",
    "    \n",
    "    \n",
    "    for batch_idx, (source_data, target_data) in enumerate(zip(trainloader, testloader)):\n",
    "#         if batch_idx > 50:\n",
    "#             break \n",
    "        str_time=time.time()\n",
    "        source_image, source_label,_name = source_data\n",
    "        target_image, target_label ,_name= target_data\n",
    "\n",
    "        p = float(batch_idx + start_steps) / total_steps\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "            #source_image = torch.cat((source_image, source_image, source_image), 1)\n",
    "\n",
    "        source_image, source_label = source_image.cuda(), source_label.cuda()\n",
    "        target_image, target_label = target_image.cuda(), target_label.cuda()\n",
    "        combined_image = torch.cat((source_image, target_image), 0)\n",
    "\n",
    "        optimizer = optim.SGD(\n",
    "            list(encoder.parameters()) +\n",
    "            list(classifier.parameters()) +\n",
    "            list(discriminator.parameters()),\n",
    "            lr=0.01,\n",
    "            momentum=0.9)\n",
    "\n",
    "        #optimizer =p3_utils.optimizer_scheduler(optimizer=optimizer, p=p)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        combined_feature = encoder(combined_image)\n",
    "        combined_feature=combined_feature.view(-1, feature_size)\n",
    "        #print(time.time()-str_time)\n",
    "        source_feature = encoder(source_image)\n",
    "        source_feature=source_feature.view(-1, feature_size)\n",
    "        #print(time.time()-str_time)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # 1.Classification loss\n",
    "        class_pred = classifier(source_feature)\n",
    "            #print(class_pred)\n",
    "        class_loss = classifier_criterion(class_pred, source_label)\n",
    "        #print(time.time()-str_time)\n",
    "            # 2. Domain loss\n",
    "        domain_pred = discriminator(combined_feature, alpha)\n",
    "        #print(time.time()-str_time)\n",
    "        domain_source_labels = torch.zeros(source_label.shape[0]).type(torch.LongTensor)\n",
    "        domain_target_labels = torch.ones(target_label.shape[0]).type(torch.LongTensor)\n",
    "        domain_combined_label = torch.cat((domain_source_labels, domain_target_labels), 0).cuda()\n",
    "        domain_loss = discriminator_criterion(domain_pred, domain_combined_label)\n",
    "    \n",
    "        total_loss = class_loss + domain_loss\n",
    "        #print(time.time()-str_time)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        print(time.time()-srt_time)\n",
    "\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print('[{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tClass Loss: {:.6f}\\tDomain Loss: {:.6f}'.format(\n",
    "                batch_idx * len(target_image), lens_use*32, 100. * batch_idx / lens_use, total_loss.item(), class_loss.item(), domain_loss.item()))\n",
    "\n",
    "    if (epoch) % 1 == 0:\n",
    "        source_acc,target_acc=test.tester(encoder, classifier, discriminator, trainloader, testloader)\n",
    "    \n",
    "    save_name='wang'\n",
    "    if(target_acc>max_target_acc):\n",
    "        save_checkpoint(encoder, classifier, discriminator,  save_name)\n",
    "        max_target_acc=target_acc\n",
    "    #visualize(encoder, 'source', save_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
